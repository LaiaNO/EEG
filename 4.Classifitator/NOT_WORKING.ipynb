{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valance arousal clasifier\n",
    "#LinearRegression\n",
    "for i in range(1,8):\n",
    "    print(\"****************************************************\")\n",
    "    print('0.'+str(i)+'************************************************')\n",
    "    numpase = 0\n",
    "    accur = []\n",
    "    xin = []\n",
    "    for t in range(0,50):\n",
    "\n",
    "        print(\"     ****************************************************\")\n",
    "        c = list(zip(X, Y))\n",
    "\n",
    "        random.shuffle(c)\n",
    "\n",
    "        DX, DY = zip(*c)\n",
    "        tnX, ttX, y_train, y_test = train_test_split(DX, DY, test_size=float('0.'+str(i)))\n",
    "        mean_data = np.array(tnX)\n",
    "        X_train = []\n",
    "        X_train1 = (mean_data[:,0] - np.mean(mean_data[:,0])) / np.std(mean_data[:,0])\n",
    "        X_train2 = (mean_data[:,1] - np.mean(mean_data[:,1])) / np.std(mean_data[:,1])\n",
    "        for i in range(0, len(X_train1)):\n",
    "            unif = []\n",
    "            unif.append(X_train1[i])\n",
    "            unif.append(X_train2[i])\n",
    "            X_train.append(unif)\n",
    "\n",
    "        mean_data = np.array(ttX)\n",
    "        X_test = []\n",
    "        X_test1 = (mean_data[:,0] - np.mean(mean_data[:,0])) / np.std(mean_data[:,0])\n",
    "        X_test2 = (mean_data[:,1] - np.mean(mean_data[:,1])) / np.std(mean_data[:,1])\n",
    "        for i in range(0, len(X_test1)):\n",
    "            unif = []\n",
    "            unif.append(X_test1[i])\n",
    "            unif.append(X_test2[i])\n",
    "            X_test.append(unif)\n",
    "        '''X_train = (tnX - np.mean(tnX)) / np.std(tnX) #STANDARIZE DATA\n",
    "        X_test = (ttX - np.mean(ttX)) / np.std(ttX) #STANDARIZE DATA'''\n",
    "\n",
    "        '''Normalize there X_train, X_test <- for across patiens mean and str (of each patient)'''\n",
    "        #plotbars and mean and str (with the results) - See visualy what is hapening\n",
    "\n",
    "        # Create an object, or instance, of the class.\n",
    "        clf = linear_model.LinearRegression()\n",
    "        # Train it\n",
    "        #fitting (learning) the parameters of a model in sklearn is fit(X, y).\n",
    "        print(clf.fit(X_train, y_train))\n",
    "        clf.coef_\n",
    "        clf.intercept_\n",
    "        clf.score(X_train, y_train)\n",
    "        #predict the class of the test data\n",
    "        #sklearn is predict(X)\n",
    "        prediction = (clf.predict(X_test))\n",
    "        if float(round(sm.mean_absolute_error(y_test, prediction)))<0.75:\n",
    "            print('     Predictet: {}\\n'.format(prediction) )\n",
    "            print('     What it should be: ', y_test)\n",
    "            # Then, compute the classification accuracy obtained.\n",
    "            print(\"     Mean absolute error =\", round(sm.mean_absolute_error(y_test, prediction), 2)) \n",
    "            print(\"     Mean squared error =\", round(sm.mean_squared_error(y_test, prediction), 2)) \n",
    "            print(\"     Median absolute error =\", round(sm.median_absolute_error(y_test, prediction), 2)) \n",
    "            print(\"     Explain variance score =\", round(sm.explained_variance_score(y_test, prediction), 2)) \n",
    "            print(\"     R2 score =\", round(sm.r2_score(y_test, prediction), 2))\n",
    "            print(' ')\n",
    "            print(' ')\n",
    "            print('     -------------- ')\n",
    "            numpase=numpase+1\n",
    "            accur.append(float(round(sm.mean_absolute_error(y_test, prediction), 2)))\n",
    "            xin.append(str(t))\n",
    "        else:\n",
    "            print('     Bad accu')\n",
    "            xin.append(str(t))\n",
    "            accur.append(float(round(sm.mean_absolute_error(y_test, prediction), 2)))\n",
    "    if numpase>25:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_axes([0,0,1,1])\n",
    "        ax.bar(xin,accur)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('************************************************ The half of the iterations dont past the .6 accur')\n",
    "    \n",
    "    print('Num de acc que pasa de 0.5: ',numpase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valance arousal clasifier\n",
    "#https://d1rwhvwstyk9gu.cloudfront.net/2020/02/XG-Boost-FINAL-01.png \n",
    "X4 = (X2 - np.mean(X2)) / np.std(X2) #STANDARIZE DATA\n",
    "for i in range(1,8):\n",
    "    print(\"****************************************************\")\n",
    "    print('0.'+str(i))\n",
    "    numpase = 0\n",
    "    accur = []\n",
    "    xin = []\n",
    "    for t in range(0,50):\n",
    "\n",
    "        print(\"     --\")\n",
    "        c = list(zip(X4, Y2))\n",
    "\n",
    "        random.shuffle(c)\n",
    "\n",
    "        DX, DY = zip(*c)\n",
    "        DX = np.array(DX)\n",
    "        DY = np.array(DY)\n",
    "        tnX, ttX, trainY, testY = train_test_split(DX, DY, test_size=float('0.'+str(i)))\n",
    "\n",
    "        mean_data = np.array(tnX)\n",
    "        trainX = []\n",
    "        X_train1 = (mean_data[:,0] - np.mean(mean_data[:,0])) / np.std(mean_data[:,0])\n",
    "        X_train2 = (mean_data[:,1] - np.mean(mean_data[:,1])) / np.std(mean_data[:,1])\n",
    "        for i in range(0, len(X_train1)):\n",
    "            unif = []\n",
    "            unif.append(X_train1[i])\n",
    "            unif.append(X_train2[i])\n",
    "            trainX.append(unif)\n",
    "        trainX = np.array(trainX)\n",
    "\n",
    "        mean_data = np.array(ttX)\n",
    "        testX = []\n",
    "        X_test1 = (mean_data[:,0] - np.mean(mean_data[:,0])) / np.std(mean_data[:,0])\n",
    "        X_test2 = (mean_data[:,1] - np.mean(mean_data[:,1])) / np.std(mean_data[:,1])\n",
    "        for i in range(0, len(X_test1)):\n",
    "            unif = []\n",
    "            unif.append(X_test1[i])\n",
    "            unif.append(X_test2[i])\n",
    "            testX.append(unif)\n",
    "        testX = np.array(testX)\n",
    "\n",
    "    \n",
    "        xgb_model = xgb.XGBClassifier(max_depth=5,learning_rate=0.1,n_estimators=50,\n",
    "                                        objective='binary:logistic',booster='gbtree',n_jobs=10,\n",
    "                                        subsample=0.9, colsample_bytree=0.9, colsample_bylevel=0.9,\n",
    "                                        reg_alpha=0.5, reg_lambda=1.0,gamma=0,\n",
    "                                        scale_pos_weight=1)\n",
    "        xgb_model.fit(trainX,trainY)\n",
    "        predY = xgb_model.predict(testX)\n",
    "        if float(accuracy_score(y_true=testY, y_pred=predY))>0.6:\n",
    "            print('     Pred: ', predY)\n",
    "            print('     Data: ', testY)\n",
    "            print(\"     ACC\",accuracy_score(y_true=testY, y_pred=predY))\n",
    "            print(\"     F1\",f1_score(y_true=testY, y_pred=predY))\n",
    "            print(\"     Recal\",recall_score(y_true=testY,y_pred=predY))\n",
    "            print(\"     Precision\",precision_score(y_true=testY, y_pred=predY))\n",
    "            numpase=numpase+1\n",
    "            accur.append(float(accuracy_score(y_true=testY, y_pred=predY)))\n",
    "            xin.append(str(t))\n",
    "        else:\n",
    "            print('     Bad accu')\n",
    "            xin.append(str(t))\n",
    "            accur.append(float(accuracy_score(y_true=testY, y_pred=predY)))\n",
    "    if numpase>25:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_axes([0,0,1,1])\n",
    "        ax.bar(xin,accur)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('************************************************ The half of the iterations dont past the .6 accur')\n",
    "    \n",
    "    print('Num de acc que pasa de 0.5: ',numpase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valance arousal clasifier\n",
    "\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "Lavels3 = []\n",
    "X3=[]\n",
    "r=0\n",
    "for i  in range(0,len(xCorr)):\n",
    "    if xCorr[i]>4.5 and r<25:\n",
    "        Lavels3.append(int(0))\n",
    "        X3.append(newdata2[i])\n",
    "        r=r+1\n",
    "    if xCorr[i]<=4.5:\n",
    "        Lavels3.append(int(1))\n",
    "        X3.append(newdata2[i])\n",
    "\n",
    "Y3 = np.array(Lavels3)\n",
    "#Data\n",
    "X3=np.array(X3)\n",
    "pca = PCA()\n",
    "components = pca.fit_transform(X3)\n",
    "labels = {\n",
    "    str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
    "    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
    "}\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    components,\n",
    "    labels=labels,\n",
    "    dimensions=range(2),\n",
    "    color=Y3\n",
    ")\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val ar\n",
    "#Linear SVC\n",
    "for i in range(1,8):\n",
    "    print(\"**************************************************** 0.\"+str(i))\n",
    "    numpase = 0\n",
    "    accur = []\n",
    "    xin = []\n",
    "    for t in range(0,50):\n",
    "\n",
    "        print(\"     --\")\n",
    "        c = list(zip(X2, Y2))\n",
    "\n",
    "        random.shuffle(c)\n",
    "\n",
    "        DX, DY = zip(*c)\n",
    "        tnX, ttX, y_train, y_test = train_test_split(DX, DY, test_size=float('0.'+str(i)))\n",
    "        mean_data = np.array(tnX)\n",
    "        X_train = []\n",
    "        X_train1 = (mean_data[:,0] - np.mean(mean_data[:,0])) / np.std(mean_data[:,0])\n",
    "        X_train2 = (mean_data[:,1] - np.mean(mean_data[:,1])) / np.std(mean_data[:,1])\n",
    "        for i in range(0, len(X_train1)):\n",
    "            unif = []\n",
    "            unif.append(X_train1[i])\n",
    "            unif.append(X_train2[i])\n",
    "            X_train.append(unif)\n",
    "\n",
    "        mean_d = np.array(ttX)\n",
    "        X_test = []\n",
    "        X_test1 = (mean_d[:,0] - np.mean(mean_data[:,0])) / np.std(mean_data[:,0])\n",
    "        X_test2 = (mean_d[:,1] - np.mean(mean_data[:,1])) / np.std(mean_data[:,1])\n",
    "        for i in range(0, len(X_test1)):\n",
    "            unif = []\n",
    "            unif.append(X_test1[i])\n",
    "            unif.append(X_test2[i])\n",
    "            X_test.append(unif)\n",
    "\n",
    "        # Create an object, or instance, of the class.\n",
    "        # Train it\n",
    "        # Adjust the parameters so it can predict the correct class as accurately.\n",
    "        clf = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.00001,\n",
    "                        C=1.0, multi_class='crammer_singer', fit_intercept=True,\n",
    "                        intercept_scaling=1, class_weight=None, verbose=0,\n",
    "                        random_state=None, max_iter=100000)\n",
    "        clf = LinearSVC(penalty='l2', multi_class='crammer_singer',)\n",
    "        if y_train.count(1)>0 and y_train.count(0)>0:\n",
    "            #fitting (learning) the parameters of a model in sklearn is fit(X, y).\n",
    "            print(clf.fit(X_train, y_train))\n",
    "            clf.coef_\n",
    "            clf.intercept_\n",
    "            clf.score(X_train, y_train)\n",
    "            #predict the class of the test data\n",
    "            #sklearn is predict(X)\n",
    "            prediction = (clf.predict(X_test))\n",
    "            try:\n",
    "                if float(accuracy_score(y_test, prediction))>0.6:\n",
    "                    print('     Predictet: {}'.format(prediction) )\n",
    "                    print('     El q ha de ser: ',y_test)\n",
    "                    # Then, compute the classification accuracy obtained.\n",
    "                    print('     Accuracy: ', accuracy_score(y_test, prediction))\n",
    "\n",
    "                    print('     Roc: ',roc_auc_score(y_test, prediction))\n",
    "\n",
    "                    print('     Recall: ',recall_score(y_test, prediction))\n",
    "                    print(' ')\n",
    "                    numpase=numpase+1\n",
    "                    accur.append(float(accuracy_score(y_test, prediction)))\n",
    "                    xin.append(str(t))\n",
    "                else:\n",
    "                    print('     Bad accu')\n",
    "        \n",
    "            except:\n",
    "                print('     Noroc curve')\n",
    "    if numpase>25:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_axes([0,0,1,1])\n",
    "        ax.bar(xin,accur)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('************************************************ The half of the iterations dont past the .6 accur')\n",
    "    print('Num de acc que pasa de 0.5: ',numpase)"
   ]
  }
 ]
}